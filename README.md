# Activity Log for AI Learning

## Additional Resources
### Papers
* Prerequiste for DETR Panoptic Segementation section
  * [(2017)[CV] Feature Pyramid Networks for Object Detection](https://arxiv.org/pdf/1612.03144.pdf)
  * [(2019)[CV] Panoptic Feature Pyramid Networks](https://arxiv.org/pdf/1901.02446.pdf)
  * [(2018)[CV] Focal Loss for Dense Object Detection](https://arxiv.org/pdf/1708.02002.pdf)
* [(2019)[CV] Self-Attention Generative Adversarial Networks](https://arxiv.org/pdf/1805.08318.pdf)
* [(2022)[NLP] PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)
* [(April 2023)[CV] Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf)
* [(April 2023)[LLM] Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442.pdf)
### Tools
* [(Apr 2023) AutoGPT](https://github.com/Torantulino/Auto-GPT)
* [(Apr 2023) BabyAGI](https://github.com/yoheinakajima/babyagi)

### Open Source LLMs
* [(Feb 2023) LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
* [(Mar 2023) Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)
* [(Mar 2023) Vicuna](https://vicuna.lmsys.org/)
* [(Apr 2023) GPT4All](https://github.com/nomic-ai/gpt4all)
* [(Apr 2023) Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
* [(Apr 2023) Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)

## April 2023

### Reading
Continued references in [UNIGE 14x050 - Deep Learning](https://fleuret.org/dlc/), [Section 13.3.	Transformer Networks](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf).
* [(2019)[NLP] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
  * [(2019)[NLP] What Does BERT Look At? An Analysis of BERT’s Attention](https://arxiv.org/pdf/1906.04341.pdf)
* [(2021)[CV] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf)
* [(2021)[CV] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030.pdf)
* [(2020)[CV] End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)
  * [(2015)[CV] (FCN) Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/pdf/1411.4038.pdf)
  * [(2015)[CV] Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)
  * [(2015)[CV] (DeepMask) Learning to Segment Object Candidates](https://arxiv.org/pdf/1506.06204.pdf)
  * [(2015)[CV] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)
  * [(2015)[CV] (ResNet) Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)
  * [(2016)[CV] (SharpMask) Learning to Refine Object Segments](https://arxiv.org/pdf/1603.08695.pdf)
  * [(2017)[CV] (FPN) Feature Pyramid Networks for Object Detection](https://arxiv.org/pdf/1612.03144.pdf)
  * [(2018)[CV] Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf)
  * [(2019)[CV] Panoptic Segmentation](https://arxiv.org/pdf/1801.00868.pdf)


### Video Lectures
Finished probability course and started statistics course.
* [Harvard Statistics 110 Probability, Lecture 27 - 34](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo)
* [MIT 18.650 Statistics for Applications, Lecture 1](https://www.youtube.com/playlist?list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0)

### Practice
* [Supervised Machine Learning: Regression and Classification](https://www.coursera.org/learn/machine-learning)


## March 2023

Continued references in [UNIGE 14x050 - Deep Learning](https://fleuret.org/dlc/), [Section 13.3.	Transformer Networks](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf).
### Reading
* [(2020)[NLP] Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361v1.pdf)
* [(2020)[NLP] Language Models are Few-Shot Learners (GPT3)](https://arxiv.org/pdf/2005.14165.pdf)
* [(2019)[NLP] Language Models are Unsupervised Multitask Learners (GPT2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf#page=12&zoom=100,0,0)
* [(2018)[NLP] Improving Language Understanding by Generative Pre-Training (GPT1)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  * [(2018)[NLP] Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/pdf/1801.10198.pdf): decoder only transformer applied to NLP
  * [(2016)[NLP] Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf): byte pair encoding
  * [(2012) Japanese and Korean Voice Search](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf): wordpiece encoding
  * [(2016)[NLP] Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf) Translation by LSTM + Attention
  * [(2016)[NLP] Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) Translation by RNN + Attention

### Video Lectures
* [Harvard Statistics 110 Probability, Lecture 4 - 26](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo)

### Practice
* [Buildspace AI Avatar Project](https://buildspace.so/builds/ai-avatar)

## February 2023

### Video Lectures
* [Harvard Statistics 110 Probability, Lecture 1 - 3](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo)
